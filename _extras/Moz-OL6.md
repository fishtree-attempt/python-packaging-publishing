---
title: "Mozilla Open Leaders Final Demo"
---

Hi, my name is Sarah Brown and I'm a postdoctoral researcher in Data Science at Brown University.

I'm building an open curriculum to teach researchers to release their python code in ways that better support repeatability and collaboration. The lesson design follows the Carpentries style: learner centric, accessible and delivered with participatory live coding.  The lesson will teach concepts of project organization, packaging, environments, documentation, and publishing.  These are topics that many researchers won't know: whether self-taught or focused on theory, these very practical aspects are not learned.

I hope my curriculum can move us from a state where a lot of research code is not released, and what much of is released is a set script that makes it easy to reproduce a paper's result, but make it hard to compare a new technique or apply a method to a new dataset.
To get there, I aim to fill a gap in training researchers with minimal practices they can adopt without requiring learning  too many specialized tools. I found that a lot of documentation and tools for these concepts focus on bigger  software projects and I think data analysis projects have slightly different nature and deserve their own support in the form of tutorials and conventions.


To date, I've built out an outline with learning objectives and started filling in some activities.  I will pilot the workshop as a full day session at Brown University in February. I would welcome more activities and examples to use in the workshop and more thoughtful conversation about what a minimal set of open source practices for this purpose could look like. I plan to make the lesson more accessible via the Carpentries Lab program, when it launches.


<!-- ## Practice Version /Notes

- Curriculum that distills minimal principles of open source
- removes barriers to better code practices
- more accessible tutorials
- delivered hands-on code-along format

My vision is that researchers will have the tools to release their code and that they will release research code to have better reproducible science and baselines and comparisons

Next steps:
- looking forward to carpentries lab
- more activities and examples
- pilot in februrary

feedback:
- good and slow, understandable
- example to fill time, scenario of what it might look like, pause
- mention python specific
- general principles, packaging, documentation,
- baseline/comparison vs reproducibiliy
- citation and reward structure for researchers
- address resource


This is to be a collaboration among scientific computing researchers and educators to develop accessible tutorial materials and minimal templates that empower researchers developing new analysis techniques to release them in formats that encourage community adoption. -->


<!--
## Week 12 Vision  Ex

What I plan to create as a leader on this project
 - curriculum
The vision of what will be so, if I am successful in my work
 - reproducible research
 - better minimum working examples
What will be so in 5/10/20 years time? What is your overall BIG vision?
 -
Why is this important to you? What is in it for you? For others?
 -
What are 2-3 key activities or critical points we should know about your vision?
 -  
What is the difference your vision will make for you, your community, the world?

Progress
- outline and some examples

Looking for
 - ideas on activities/examples
 - exercises for practice

Crowd AI as a possible resource -->

<!--
I am building an open curriculum to prepare researchers to release the python data analysis projects and tools in ways that better support repeatability and comparison of methods. The lesson is designed to be delivered in the participatory live coding that is a signature of the Carpentries.
 <!-- means that the material is novice-friendly, learner-centric, and intended to be delivered in a live-code-along format. -->
It focuses on researchers by anchoring on the core principles of open source that will provide benefit without too much unrewarded overhead to researchers. A focus on repeatability and comparison, is a step beyond reproducibility into a realm of more collaborative research and emphasizes documentation, packaging, and usability in a different way that a set of scripts that can be run, but not understood. -->
